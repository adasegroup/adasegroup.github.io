---
layout: project_page
team: dlpa
categories: dlpa

front_id: 3

title: Multi-fidelity Neural Architecture Search with Knowledge Distillation
authors:
  - name: Ilya Trofimov
    affiliation: 1
  - name: Nikita Klyuchnikov
    affiliation: 1
  - name: Mikhail Salnikov
    affiliation: 1
  - name: Alexander Filippov
    affiliation: 2
  - name: Evgeny Burnaev
    affiliation: 1
affiliation:
  - Skolkovo Institute of Science and Technology
  - Huawei Noah's Ark Lab
venue: arXiv 2020

excerpt: In this work we propose to speedup Neural architecture search (NAS) by making low-fidelity evaluations of neural architectures with the knowledge distillaion.
abstract: Neural architecture search (NAS) targets at finding the optimal architecture of a neural network for a problem or a family of problems. Evaluations of neural architectures are very time-consuming. One of the possible ways to mitigate this issue is to use low-fidelity evaluations, namely training on a part of a dataset, fewer epochs, with fewer channels, etc. In this paper, we propose to improve low-fidelity evaluations of neural architectures by using a knowledge distillation. Knowledge distillation adds to a loss function a term forcing a network to mimic some teacher network. We carry out experiments on CIFAR-100 and ImageNet and study various knowledge distillation methods. We show that training on the small part of a dataset with such a modified loss function leads to a better selection of neural architectures than training with a logistic loss. The proposed low-fidelity evaluations were incorporated into a multi-fidelity search algorithm that outperformed the search based on high-fidelity evaluations only (training on a full dataset).

thumbnail: /assets/img/projects/mf_nas_kd/mf_kd_nas_thumb.jpg
visual_abstract: /assets/img/projects/mf_nas_kd/teaser-pic.jpg
visual_abstract_description: 'Pearson correlation between high-fidelity and low-fidelity evaluations of architectures.'
bibtex: "\
@article{trofimov2020multi,
  title={Multi-fidelity Neural Architecture Search with Knowledge Distillation},
  author={Trofimov, Ilya and Klyuchnikov, Nikita and Salnikov, Mikhail and Filippov, Alexander and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:2006.08341},
  year={2020}
}"

materials:
    - name: Paper
      url: https://arxiv.org/pdf/2006.08341.pdf
      icon: fa fa-file-pdf-o
    - name: Code
      url: https://github.com/IlyaTrofimov/MF_NAS_KD
      icon: fa fa-github-alt
---
